# South Punjab Flood Prediction - Machine Learning Implementation

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
import warnings
import os
from scipy import stats
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class SouthPunjabFloodPredictor:
    
    def __init__(self, data_path="/Users/shahmeer/Desktop/flood/csv_files/"): '''change directory, this is according to my system'''
        self.data_path = data_path
        self.results = {}
        self.climate_trends = {}
        self.ml_results = {}
        
    def load_data(self):
        filepath = f"{self.data_path}SouthPunjab_FloodTraining_2000_2025.csv"
        self.df_main = pd.read_csv(filepath)
        
        if 'EXP2_YEAR' in self.df_main.columns:
            self.df_main['EXP2_YEAR'] = self.df_main['Year'].astype(int)
            
        if 'Flood_Occurred' in self.df_main.columns:
            self.df_main['Flood_Binary'] = (self.df_main['Flood_Occurred'] > 0.3).astype(int)
            
        return self.df_main
    
    def analyze_climate_trends(self):
        climate_vars = ['Rainfall_Total', 'NDVI', 'LST_Mean']
        
        for var in climate_vars:
            if var in self.df_main.columns:
                annual_data = self.df_main.groupby('Year')[var].mean().reset_index()
                
                slope, intercept, r_value, p_value, std_err = stats.linregress(
                    annual_data['Year'], annual_data[var]
                )
                
                trend_per_decade = slope * 10
                total_change = slope * 25
                r_squared = r_value ** 2
                significance = "Significant" if p_value < 0.05 else "Not significant"
                
                self.climate_trends[var] = {
                    'slope': slope,
                    'trend_per_decade': trend_per_decade,
                    'total_change': total_change,
                    'r_squared': r_squared,
                    'p_value': p_value,
                    'significance': significance
                }
        
        return self.climate_trends
    
    def prepare_features(self):
        self.annual_data = self.df_main.groupby(['Year', 'ADM2_NAME']).agg({
            'Rainfall_Total': 'mean',
            'Rainfall_Max': 'mean', 
            'Rainfall_Mean': 'mean',
            'NDVI': 'mean',
            'LST_Mean': 'mean',
            'Elevation': 'mean',
            'Slope': 'mean',
            'Water_Distance': 'mean',
            'Soil_Texture': 'mean',
            'Flood_Occurred': 'max',
            'Flood_Binary': 'max'
        }).reset_index()
        
        self.annual_data['Rainfall_Intensity'] = (
            self.annual_data['Rainfall_Max'] / self.annual_data['Rainfall_Mean']
        )
        self.annual_data['Rainfall_Anomaly'] = (
            (self.annual_data['Rainfall_Total'] - self.annual_data['Rainfall_Total'].mean()) / 
            self.annual_data['Rainfall_Total'].std()
        )
        self.annual_data['Year_Normalized'] = (
            (self.annual_data['Year'] - self.annual_data['Year'].min()) / 
            (self.annual_data['Year'].max() - self.annual_data['Year'].min())
        )
        
        self.annual_data['Climate_Risk_Index'] = (
            0.4 * self.annual_data['Rainfall_Anomaly'] + 
            0.3 * (1 - self.annual_data['NDVI']) + 
            0.3 * self.annual_data['LST_Mean'] / self.annual_data['LST_Mean'].max()
        )
        
        rainfall_90th = np.percentile(self.annual_data['Rainfall_Total'], 90)
        rainfall_85th = np.percentile(self.annual_data['Rainfall_Total'], 85)
        rainfall_80th = np.percentile(self.annual_data['Rainfall_Total'], 80)
        
        self.annual_data['Historical_Climate_Flood'] = self.annual_data['Flood_Binary']
        self.annual_data['Flood_Rainfall_90th'] = (self.annual_data['Rainfall_Total'] > rainfall_90th).astype(int)
        self.annual_data['Flood_Rainfall_85th'] = (self.annual_data['Rainfall_Total'] > rainfall_85th).astype(int)
        self.annual_data['Flood_Rainfall_80th'] = (self.annual_data['Rainfall_Total'] > rainfall_80th).astype(int)
        
        return self.annual_data
    
    def train_ml_models(self):
        climate_features = [
            'Rainfall_Total', 'Rainfall_Max', 'Rainfall_Mean', 'Rainfall_Intensity',
            'Rainfall_Anomaly', 'Climate_Risk_Index', 'Year_Normalized', 
            'Elevation', 'Slope', 'Water_Distance', 'NDVI', 'LST_Mean'
        ]
        
        available_features = [f for f in climate_features if f in self.annual_data.columns]
        
        X = self.annual_data[available_features].copy()
        X = X.fillna(X.median())
        
        scaler = StandardScaler()
        X_scaled = pd.DataFrame(
            scaler.fit_transform(X), 
            columns=X.columns, 
            index=X.index
        )
        
        targets = {
            'Historical Floods': self.annual_data['Historical_Climate_Flood'],
            'Rainfall 90th Percentile': self.annual_data['Flood_Rainfall_90th'],
            'Rainfall 85th Percentile': self.annual_data['Flood_Rainfall_85th'],
            'Rainfall 80th Percentile': self.annual_data['Flood_Rainfall_80th']
        }
        
        models = {
            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
            'SVM (RBF)': SVC(random_state=42, probability=True, kernel='rbf'),
            'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100)
        }
        
        self.ml_results = {}
        
        for target_name, y in targets.items():
            if y.sum() >= 2:
                X_train, X_test, y_train, y_test = train_test_split(
                    X_scaled, y, test_size=0.2, random_state=42, stratify=y
                )
                
                target_results = {}
                
                for model_name, model in models.items():
                    model.fit(X_train, y_train)
                    
                    y_pred = model.predict(X_test)
                    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
                    
                    accuracy = accuracy_score(y_test, y_pred)
                    precision = precision_score(y_test, y_pred, zero_division=0)
                    recall = recall_score(y_test, y_pred, zero_division=0)
                    f1 = f1_score(y_test, y_pred, zero_division=0)
                    auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else 0
                    
                    cv = StratifiedKFold(n_splits=min(5, y.sum()), shuffle=True, random_state=42)
                    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
                    
                    target_results[model_name] = {
                        'Accuracy': accuracy,
                        'Precision': precision,
                        'Recall': recall,
                        'F1-Score': f1,
                        'AUC': auc,
                        'CV_Mean': cv_scores.mean(),
                        'CV_Std': cv_scores.std()
                    }
                
                self.ml_results[target_name] = target_results
        
        return self.ml_results
    
    def create_district_risk_assessment(self):
        self.district_risk = self.annual_data.groupby('ADM2_NAME').agg({
            'Climate_Risk_Index': 'mean',
            'Rainfall_Total': 'mean',
            'NDVI': 'mean',
            'LST_Mean': 'mean',
            'Historical_Climate_Flood': 'sum',
            'Flood_Rainfall_90th': 'sum',
            'Flood_Rainfall_85th': 'sum',
            'Flood_Rainfall_80th': 'sum'
        }).reset_index()
        
        self.district_risk.columns = [
            'District', 'Avg_Climate_Risk', 'Avg_Rainfall_mm', 'Avg_NDVI', 
            'Avg_Temperature_C', 'Historical_Floods', 'High_Rainfall_Events_90th',
            'High_Rainfall_Events_85th', 'High_Rainfall_Events_80th'
        ]
        
        self.district_risk = self.district_risk.sort_values('Avg_Climate_Risk', ascending=False)
        self.district_risk['Risk_Rank'] = range(1, len(self.district_risk) + 1)
        
        return self.district_risk
    
    def export_results(self, output_dir="Final_Results"):
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # Climate trends
        climate_trends_df = pd.DataFrame([
            {
                'Variable': var,
                'Annual_Trend_per_year': data['slope'],
                'Trend_per_Decade': data['trend_per_decade'],
                'Total_25yr_Change': data['total_change'],
                'R_Squared': data['r_squared'],
                'P_Value': data['p_value'],
                'Statistical_Significance': data['significance']
            }
            for var, data in self.climate_trends.items()
        ])
        climate_trends_df.to_csv(f"{output_dir}/Climate_Trends_Analysis.csv", index=False)
        
        # ML performance
        ml_performance_data = []
        for target, models in self.ml_results.items():
            for model, metrics in models.items():
                ml_performance_data.append({
                    'Target_Classification': target,
                    'ML_Model': model,
                    'Accuracy': f"{metrics['Accuracy']:.3f}",
                    'Precision': f"{metrics['Precision']:.3f}",
                    'Recall': f"{metrics['Recall']:.3f}",
                    'F1_Score': f"{metrics['F1-Score']:.3f}",
                    'AUC': f"{metrics['AUC']:.3f}",
                    'CV_Mean': f"{metrics['CV_Mean']:.3f}",
                    'CV_Std': f"{metrics['CV_Std']:.3f}"
                })
        
        ml_performance_df = pd.DataFrame(ml_performance_data)
        ml_performance_df.to_csv(f"{output_dir}/ML_Performance_Results.csv", index=False)
        
        # District risk assessment
        self.district_risk.to_csv(f"{output_dir}/District_Risk_Assessment.csv", index=False)
        
        # Summary statistics
        summary_stats = {
            'Metric': [
                'Study Period', 'Districts Analyzed', 'Total Samples', 'Climate Features',
                'ML Models Tested', 'Best Accuracy Achieved', 'Best F1-Score', 'Best AUC',
                'Rainfall Trend (mm/year)', 'NDVI Trend (/year)', 'Temperature Trend (°C/year)',
                'Highest Risk District', 'Lowest Risk District'
            ],
            'Value': [
                f"{self.annual_data['Year'].min()}-{self.annual_data['Year'].max()}",
                f"{self.annual_data['ADM2_NAME'].nunique()}",
                f"{len(self.annual_data)}",
                f"{len([f for f in self.annual_data.columns if 'Rainfall' in f or 'NDVI' in f or 'LST' in f])}",
                f"{len(['Logistic Regression', 'SVM', 'Random Forest'])}",
                f"{max([max([m['Accuracy'] for m in target.values()]) for target in self.ml_results.values()]):.3f}",
                f"{max([max([m['F1-Score'] for m in target.values()]) for target in self.ml_results.values()]):.3f}",
                f"{max([max([m['AUC'] for m in target.values()]) for target in self.ml_results.values()]):.3f}",
                f"{self.climate_trends['Rainfall_Total']['slope']:+.3f}" if 'Rainfall_Total' in self.climate_trends else 'N/A',
                f"{self.climate_trends['NDVI']['slope']:+.4f}" if 'NDVI' in self.climate_trends else 'N/A',
                f"{self.climate_trends['LST_Mean']['slope']:+.3f}" if 'LST_Mean' in self.climate_trends else 'N/A',
                f"{self.district_risk.iloc[0]['District']}",
                f"{self.district_risk.iloc[-1]['District']}"
            ]
        }
        
        summary_df = pd.DataFrame(summary_stats)
        summary_df.to_csv(f"{output_dir}/Project_Summary_Statistics.csv", index=False)
        
        return output_dir

def main():
    predictor = SouthPunjabFloodPredictor()
    
    predictor.load_data()
    predictor.analyze_climate_trends()
    predictor.prepare_features()
    predictor.train_ml_models()
    predictor.create_district_risk_assessment()
    output_dir = predictor.export_results()
    
    return predictor, output_dir

if __name__ == "__main__":
    predictor, results_dir = main()

-----------------------------------------------------------------
  import ee
import pandas as pd
from datetime import datetime, timedelta

# Initialize Earth Engine
try:
    ee.Initialize()
    print("Earth Engine initialized successfully")
except Exception as e:
    print(f"Earth Engine initialization failed: {e}")
    print("Please run: earthengine authenticate")
    exit(1)

# =========================
# 1️⃣ Define Study Area
# =========================
south_punjab_districts = [
    'Multan District', 'Bahawalpur District', 'Bahawalnagar District', 'Rahim Yar Khan District',
    'Lodhran District', 'Dera Ghazi Khan District', 'Muzaffargarh District', 'Rajanpur District',
    'Vehari District', 'Khanewal District', 'Layyah District'
]

districts = (ee.FeatureCollection("FAO/GAUL/2015/level2")
             .filter(ee.Filter.eq('ADM1_NAME', 'Punjab'))
             .filter(ee.Filter.inList('ADM2_NAME', south_punjab_districts)))

# Check if districts are found
district_count = districts.size()
print(f'Number of South Punjab districts found: {district_count.getInfo()}')
print(f'South Punjab district names: {districts.aggregate_array("ADM2_NAME").getInfo()}')

# =========================
# 2️⃣ Define Time Period
# =========================
start_date = '2024-06-01'
end_date = '2024-09-30'

# Pre-monsoon period for antecedent conditions
pre_monsoon_start = '2024-05-01'
pre_monsoon_end = '2024-05-31'

print(f"Analysis period: {start_date} to {end_date}")
print(f"Pre-monsoon baseline: {pre_monsoon_start} to {pre_monsoon_end}")

# =========================
# 3️⃣ ENHANCED PRECIPITATION FEATURES
# =========================

# Comprehensive rainfall statistics
rainfall_collection = (ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY')
                       .filterDate(start_date, end_date)
                       .select('precipitation'))

rainfall_stats = rainfall_collection.reduce(
    ee.Reducer.mean()
    .combine(ee.Reducer.max(), '', True)
    .combine(ee.Reducer.stdDev(), '', True)
    .combine(ee.Reducer.sum(), '', True)
)

# Monthly rainfall distribution
june_rain = rainfall_collection.filterDate('2024-06-01', '2024-06-30').sum()
july_rain = rainfall_collection.filterDate('2024-07-01', '2024-07-31').sum()
aug_rain = rainfall_collection.filterDate('2024-08-01', '2024-08-31').sum()
sep_rain = rainfall_collection.filterDate('2024-09-01', '2024-09-30').sum()

# Extreme precipitation events (days with >50mm)
extreme_events = rainfall_collection.map(lambda img: img.gt(50)).sum()

print("Precipitation features extracted")

# =========================
# 4️⃣ ENHANCED VEGETATION & TEMPERATURE
# =========================

def apply_ndvi_quality_mask(img):
    """Apply quality mask for MODIS NDVI"""
    qa = img.select('SummaryQA')
    quality_mask = qa.bitwiseAnd(3).eq(0)  # Good quality pixels
    
    return (img.select('NDVI')
            .multiply(0.0001)
            .updateMask(quality_mask))

# NDVI with temporal statistics and quality masking
ndvi_collection = (ee.ImageCollection('MODIS/061/MOD13Q1')
                   .filterDate(start_date, end_date)
                   .map(apply_ndvi_quality_mask))

ndvi_stats = ndvi_collection.reduce(
    ee.Reducer.mean()
    .combine(ee.Reducer.min(), '', True)
    .combine(ee.Reducer.max(), '', True)
)

# Monthly NDVI for temporal trends
ndvi_june = ndvi_collection.filterDate('2024-06-01', '2024-06-30').mean()
ndvi_july = ndvi_collection.filterDate('2024-07-01', '2024-07-31').mean()
ndvi_aug = ndvi_collection.filterDate('2024-08-01', '2024-08-31').mean()
ndvi_sep = ndvi_collection.filterDate('2024-09-01', '2024-09-30').mean()

def process_lst(img):
    """Process LST - convert to Celsius"""
    return (img.select('LST_Day_1km')
            .multiply(0.02)
            .subtract(273.15))

# LST with enhanced statistics
lst_collection = (ee.ImageCollection('MODIS/061/MOD11A2')
                  .filterDate(start_date, end_date)
                  .map(process_lst))

lst_day_stats = lst_collection.reduce(
    ee.Reducer.mean()
    .combine(ee.Reducer.max(), '', True)
    .combine(ee.Reducer.min(), '', True)
)

# Monthly LST for temporal trends
lst_june = lst_collection.filterDate('2024-06-01', '2024-06-30').mean()
lst_july = lst_collection.filterDate('2024-07-01', '2024-07-31').mean()
lst_aug = lst_collection.filterDate('2024-08-01', '2024-08-31').mean()
lst_sep = lst_collection.filterDate('2024-09-01', '2024-09-30').mean()

print("Vegetation and temperature features extracted")

# =========================
# 5️⃣ CRITICAL TERRAIN FEATURES
# =========================

# Elevation and derived terrain features
elevation = ee.Image('USGS/SRTMGL1_003')
slope = ee.Terrain.slope(elevation)
aspect = ee.Terrain.aspect(elevation)

print("Terrain features extracted")

# =========================
# 6️⃣ HYDROLOGICAL FEATURES
# =========================

# Distance to water bodies using Global Surface Water
water_occurrence = ee.Image("JRC/GSW1_3/GlobalSurfaceWater").select('occurrence')

# Permanent water bodies (>75% occurrence)
permanent_water = water_occurrence.gt(75)
water_distance = (permanent_water.fastDistanceTransform().sqrt()
                  .multiply(ee.Image.pixelArea().sqrt()))

# Flow accumulation (proxy for drainage network)
flow_accumulation = ee.Image("WWF/HydroSHEDS/03VFDEM").select('b1')

print("Hydrological features extracted")

# =========================
# 7️⃣ SOIL AND ANTECEDENT CONDITIONS
# =========================

# Soil texture - select the main texture class band
soil_texture = ee.Image('OpenLandMap/SOL/SOL_TEXTURE-CLASS_USDA-TT_M/v02').select('b0')

# Pre-monsoon soil moisture (if available)
pre_monsoon_sm = (ee.ImageCollection('NASA_USDA/HSL/SMAP10KM_soil_moisture')
                  .filterDate(pre_monsoon_start, pre_monsoon_end)
                  .select('ssm')
                  .mean())

print("Soil and antecedent conditions extracted")

# =========================
# 8️⃣ LAND USE/LAND COVER
# =========================

# ESA WorldCover for land use classification
landcover = ee.ImageCollection("ESA/WorldCover/v200").first().select('Map')

print("Land cover data extracted")

# =========================
# 9️⃣ COMBINE ALL FEATURES WITH PROJECTION CONSISTENCY
# =========================

# Define common projection and scale for consistency
target_projection = 'EPSG:4326'  # WGS84
target_scale = 500  # meters

def reproject_band(image, band_name, new_name):
    """Helper function to reproject and rename bands"""
    return image.select(band_name).rename(new_name).reproject(target_projection, None, target_scale)

# Build enhanced feature stack
enhanced_stack = (
    reproject_band(rainfall_stats, 'precipitation_mean', 'Rainfall_Mean_mm')
    .addBands(reproject_band(rainfall_stats, 'precipitation_max', 'Rainfall_Max_mm'))
    .addBands(reproject_band(rainfall_stats, 'precipitation_stdDev', 'Rainfall_StdDev_mm'))
    .addBands(reproject_band(rainfall_stats, 'precipitation_sum', 'Rainfall_Total_mm'))
    .addBands(reproject_band(june_rain, 'precipitation', 'June_Rain_mm'))
    .addBands(reproject_band(july_rain, 'precipitation', 'July_Rain_mm'))
    .addBands(reproject_band(aug_rain, 'precipitation', 'Aug_Rain_mm'))
    .addBands(reproject_band(sep_rain, 'precipitation', 'Sep_Rain_mm'))
    .addBands(reproject_band(extreme_events, 'precipitation', 'Extreme_Rain_Days'))
    .addBands(reproject_band(ndvi_stats, 'NDVI_mean', 'NDVI_Mean'))
    .addBands(reproject_band(ndvi_stats, 'NDVI_min', 'NDVI_Min'))
    .addBands(reproject_band(ndvi_stats, 'NDVI_max', 'NDVI_Max'))
    .addBands(reproject_band(ndvi_june, 'NDVI', 'NDVI_June'))
    .addBands(reproject_band(ndvi_july, 'NDVI', 'NDVI_July'))
    .addBands(reproject_band(ndvi_aug, 'NDVI', 'NDVI_Aug'))
    .addBands(reproject_band(ndvi_sep, 'NDVI', 'NDVI_Sep'))
    .addBands(reproject_band(lst_day_stats, 'LST_Day_1km_mean', 'LST_Day_Mean_C'))
    .addBands(reproject_band(lst_day_stats, 'LST_Day_1km_max', 'LST_Day_Max_C'))
    .addBands(reproject_band(lst_day_stats, 'LST_Day_1km_min', 'LST_Day_Min_C'))
    .addBands(reproject_band(lst_june, 'LST_Day_1km', 'LST_June_C'))
    .addBands(reproject_band(lst_july, 'LST_Day_1km', 'LST_July_C'))
    .addBands(reproject_band(lst_aug, 'LST_Day_1km', 'LST_Aug_C'))
    .addBands(reproject_band(lst_sep, 'LST_Day_1km', 'LST_Sep_C'))
    .addBands(elevation.rename('Elevation_m').reproject(target_projection, None, target_scale))
    .addBands(slope.rename('Slope_degrees').reproject(target_projection, None, target_scale))
    .addBands(aspect.rename('Aspect_degrees').reproject(target_projection, None, target_scale))
    .addBands(water_distance.rename('Water_Distance_m').reproject(target_projection, None, target_scale))
    .addBands(flow_accumulation.rename('Flow_Accumulation').reproject(target_projection, None, target_scale))
    .addBands(soil_texture.rename('Soil_Texture_Class').reproject(target_projection, None, target_scale))
    .addBands(landcover.rename('Land_Cover_Class').reproject(target_projection, None, target_scale))
)

# Add soil moisture if available
if pre_monsoon_sm.bandNames().size().gt(0):
    enhanced_stack = enhanced_stack.addBands(
        pre_monsoon_sm.rename('PreMonsoon_SoilMoisture').reproject(target_projection, None, target_scale)
    )

# Clip to study area
enhanced_stack = enhanced_stack.clip(districts)

print("Feature stack created with consistent projection")

# =========================
# 🔟 ENHANCED ZONAL STATISTICS WITH CONSISTENT SCALING
# =========================
enhanced_stats = enhanced_stack.reduceRegions(
    collection=districts,
    reducer=(ee.Reducer.mean()
             .combine(ee.Reducer.minMax(), '', True)
             .combine(ee.Reducer.stdDev(), '', True)),
    scale=target_scale,
    crs=target_projection
)

print("Zonal statistics calculated")

# =========================
# 1️⃣1️⃣ EXPORT ENHANCED DATASET
# =========================
def export_to_drive(collection, description, folder='FloodPrediction'):
    """Export feature collection to Google Drive"""
    task = ee.batch.Export.table.toDrive(
        collection=collection,
        description=description,
        fileFormat='CSV',
        folder=folder
    )
    task.start()
    print(f"Export started: {description}")
    return task

# Export current year dataset
export_task_2024 = export_to_drive(
    enhanced_stats, 
    'Enhanced_SouthPunjab_FloodPrediction_2024'
)

# =========================
# 1️⃣2️⃣ FLOOD DETECTION FUNCTIONS
# =========================

def detect_floods_sentinel1(start_date, end_date, aoi):
    """Detect floods using Sentinel-1 SAR data"""
    
    def process_s1_image(img):
        """Process Sentinel-1 image - use VH if available, otherwise VV"""
        bands = img.bandNames()
        has_vh = bands.contains('VH')
        return ee.Algorithms.If(has_vh, img.select('VH'), img.select('VV'))
    
    # Get Sentinel-1 data before flood period
    s1_before = (ee.ImageCollection('COPERNICUS/S1_GRD')
                 .filterBounds(aoi)
                 .filterDate(ee.Date(start_date).advance(-30, 'day'), start_date)
                 .filter(ee.Filter.eq('instrumentMode', 'IW'))
                 .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))
                 .map(process_s1_image)
                 .mean())
    
    # Get Sentinel-1 data during flood period
    s1_during = (ee.ImageCollection('COPERNICUS/S1_GRD')
                 .filterBounds(aoi)
                 .filterDate(start_date, end_date)
                 .filter(ee.Filter.eq('instrumentMode', 'IW'))
                 .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))
                 .map(process_s1_image)
                 .mean())
    
    # Check if we have data
    has_data = s1_before.bandNames().size().gt(0).And(s1_during.bandNames().size().gt(0))
    
    # Calculate difference only if we have data
    flood_mask = ee.Algorithms.If(
        has_data,
        ee.Image(s1_during.subtract(s1_before).lt(-3)),  # Flood threshold: -3 dB
        ee.Image.constant(0)  # Return 0 if no data
    )
    
    return ee.Image(flood_mask)

def detect_floods_modis(start_date, end_date, aoi):
    """Detect floods using MODIS Surface Reflectance"""
    
    # Pre-flood baseline (dry season)
    baseline = (ee.ImageCollection('MODIS/061/MOD09GA')
                .filterBounds(aoi)
                .filterDate(ee.Date(start_date).advance(-60, 'day'), 
                           ee.Date(start_date).advance(-30, 'day'))
                .select(['sur_refl_b01', 'sur_refl_b02'])  # Red, NIR
                .mean())
    
    # During flood period
    flood_period = (ee.ImageCollection('MODIS/061/MOD09GA')
                    .filterBounds(aoi)
                    .filterDate(start_date, end_date)
                    .select(['sur_refl_b01', 'sur_refl_b02'])
                    .mean())
    
    # Check if we have data
    has_data = baseline.bandNames().size().gt(0).And(flood_period.bandNames().size().gt(0))
    
    # Calculate NDWI (Normalized Difference Water Index) only if we have data
    flood_mask = ee.Algorithms.If(
        has_data,
        ee.Image(flood_period.normalizedDifference(['sur_refl_b01', 'sur_refl_b02'])
                 .subtract(baseline.normalizedDifference(['sur_refl_b01', 'sur_refl_b02']))
                 .gt(0.2)),  # Threshold for water detection
        ee.Image.constant(0)  # Return 0 if no data
    )
    
    return ee.Image(flood_mask)

print("Flood detection functions defined")

# =========================
# 1️⃣3️⃣ HISTORICAL FLOOD EVENTS (2000-2025)
# =========================

# Define historical flood years and dates for South Punjab (25 years of data)
historical_floods = [
    # 2000-2009 decade
    {'year': '2000', 'start': '2000-07-01', 'end': '2000-08-31', 'label': 'Flood_2000'},
    {'year': '2001', 'start': '2001-07-15', 'end': '2001-08-15', 'label': 'Flood_2001'},
    {'year': '2002', 'start': '2002-06-15', 'end': '2002-09-15', 'label': 'Flood_2002'},
    {'year': '2003', 'start': '2003-07-01', 'end': '2003-08-31', 'label': 'Flood_2003'},
    {'year': '2004', 'start': '2004-07-20', 'end': '2004-08-20', 'label': 'Flood_2004'},
    {'year': '2005', 'start': '2005-06-15', 'end': '2005-09-15', 'label': 'Flood_2005'},
    {'year': '2006', 'start': '2006-07-01', 'end': '2006-08-31', 'label': 'Flood_2006'},
    {'year': '2007', 'start': '2007-07-15', 'end': '2007-08-15', 'label': 'Flood_2007'},
    {'year': '2008', 'start': '2008-06-15', 'end': '2008-09-15', 'label': 'Flood_2008'},
    {'year': '2009', 'start': '2009-07-01', 'end': '2009-08-31', 'label': 'Flood_2009'},
    
    # 2010-2019 decade (including major flood years)
    {'year': '2010', 'start': '2010-07-15', 'end': '2010-09-15', 'label': 'Flood_2010'},  # Major floods
    {'year': '2011', 'start': '2011-08-01', 'end': '2011-09-30', 'label': 'Flood_2011'},  # Major floods
    {'year': '2012', 'start': '2012-07-01', 'end': '2012-08-31', 'label': 'Flood_2012'},  # Major floods
    {'year': '2013', 'start': '2013-07-15', 'end': '2013-08-15', 'label': 'Flood_2013'},
    {'year': '2014', 'start': '2014-09-01', 'end': '2014-09-30', 'label': 'Flood_2014'},  # Major floods
    {'year': '2015', 'start': '2015-07-01', 'end': '2015-08-31', 'label': 'Flood_2015'},  # Major floods
    {'year': '2016', 'start': '2016-07-20', 'end': '2016-08-20', 'label': 'Flood_2016'},
    {'year': '2017', 'start': '2017-08-01', 'end': '2017-09-15', 'label': 'Flood_2017'},  # Major floods
    {'year': '2018', 'start': '2018-07-01', 'end': '2018-08-31', 'label': 'Flood_2018'},
    {'year': '2019', 'start': '2019-07-15', 'end': '2019-08-15', 'label': 'Flood_2019'},
    
    # 2020-2025 recent years
    {'year': '2020', 'start': '2020-07-01', 'end': '2020-08-31', 'label': 'Flood_2020'},
    {'year': '2021', 'start': '2021-07-20', 'end': '2021-08-20', 'label': 'Flood_2021'},
    {'year': '2022', 'start': '2022-06-15', 'end': '2022-09-15', 'label': 'Flood_2022'},  # Major floods
    {'year': '2023', 'start': '2023-07-01', 'end': '2023-08-31', 'label': 'Flood_2023'},
    {'year': '2024', 'start': '2024-06-01', 'end': '2024-09-30', 'label': 'Flood_2024'},
    {'year': '2025', 'start': '2025-06-01', 'end': '2025-09-30', 'label': 'Flood_2025'}
]

known_flood_years = ['2010', '2011', '2012', '2014', '2015', '2017', '2022']
print(f'Known major flood years in South Punjab: {known_flood_years}')
print('Historical training years: 2000-2025 (25 years)')

# =========================
# 1️⃣4️⃣ CREATE TRAINING DATASET FUNCTION
# =========================

def create_training_dataset(flood_event, districts):
    """Create training dataset with flood labels for a specific year"""
    year = flood_event['year']
    start_date = flood_event['start']
    end_date = flood_event['end']
    monsoon_start = f"{year}-06-01"
    monsoon_end = f"{year}-09-30"
    
    # Extract features for this year
    yearly_rainfall = (ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY')
                       .filterDate(monsoon_start, monsoon_end)
                       .select('precipitation')
                       .reduce(ee.Reducer.mean()
                               .combine(ee.Reducer.max(), '', True)
                               .combine(ee.Reducer.sum(), '', True)))
    
    def process_yearly_ndvi(img):
        """Simplified NDVI processing for historical data"""
        return img.select('NDVI').multiply(0.0001)
    
    yearly_ndvi = (ee.ImageCollection('MODIS/061/MOD13Q1')
                   .filterDate(monsoon_start, monsoon_end)
                   .select('NDVI')
                   .map(process_yearly_ndvi)
                   .mean())
    
    def process_yearly_lst(img):
        """Simplified LST processing for historical data"""
        return img.select('LST_Day_1km').multiply(0.02).subtract(273.15)
    
    yearly_lst = (ee.ImageCollection('MODIS/061/MOD11A2')
                  .filterDate(monsoon_start, monsoon_end)
                  .select('LST_Day_1km')
                  .map(process_yearly_lst)
                  .mean())
    
    # Determine flood detection method based on year
    year_num = ee.Number.parse(year)
    
    # Use Sentinel-1 for 2014+ (more accurate), MODIS for 2000-2013
    flood_mask = ee.Algorithms.If(
        year_num.gte(2014),
        detect_floods_sentinel1(start_date, end_date, districts),
        detect_floods_modis(start_date, end_date, districts)
    )
    flood_mask = ee.Image(flood_mask)
    
    # Create comprehensive feature stack for this year
    yearly_features = (yearly_rainfall.select('precipitation_mean').rename('Rainfall_Mean')
                       .addBands(yearly_rainfall.select('precipitation_max').rename('Rainfall_Max'))
                       .addBands(yearly_rainfall.select('precipitation_sum').rename('Rainfall_Total'))
                       .addBands(yearly_ndvi.rename('NDVI'))
                       .addBands(yearly_lst.rename('LST_Mean'))
                       .addBands(elevation.rename('Elevation'))
                       .addBands(slope.rename('Slope'))
                       .addBands(water_distance.rename('Water_Distance'))
                       .addBands(soil_texture.rename('Soil_Texture'))
                       .addBands(flood_mask.rename('Flood_Occurred'))
                       .clip(districts))
    
    # Calculate zonal statistics with flood labels
    zonal_stats = yearly_features.reduceRegions(
        collection=districts,
        reducer=ee.Reducer.mean(),
        scale=1000
    )
    
    # Add year label and metadata
    def add_metadata(feature):
        return feature.set({
            'Year': year,
            'Decade': ee.String(year).slice(0, 3).cat('0s'),
            'FloodPeriod': ee.String(start_date).cat(' to ').cat(end_date)
        })
    
    zonal_stats = zonal_stats.map(add_metadata)
    
    return zonal_stats

print("Training dataset function defined")

# =========================
# 1️⃣5️⃣ CREATE COMPREHENSIVE TRAINING DATASET
# =========================

def create_all_training_data():
    """Generate training data for all historical years"""
    print("Creating training datasets for all historical years...")
    
    # Generate training data for all years
    training_datasets = []
    for flood_event in historical_floods:
        try:
            dataset = create_training_dataset(flood_event, districts)
            training_datasets.append(dataset)
            print(f"Created dataset for {flood_event['year']}")
        except Exception as e:
            print(f"⚠️ Failed to create dataset for {flood_event['year']}: {e}")
    
    # Combine all years into single training dataset
    complete_training_data = ee.FeatureCollection(training_datasets).flatten()
    
    return complete_training_data

# Create the complete training dataset
complete_training_data = create_all_training_data()

# Export historical training dataset (2000-2025)
export_task_complete = export_to_drive(
    complete_training_data,
    'SouthPunjab_FloodTraining_2000_2025'
)

# Export by decades for analysis
training_2000s = complete_training_data.filter(ee.Filter.stringContains('Decade', '200'))
training_2010s = complete_training_data.filter(ee.Filter.stringContains('Decade', '201'))
training_2020s = complete_training_data.filter(ee.Filter.stringContains('Decade', '202'))

export_task_2000s = export_to_drive(training_2000s, 'SouthPunjab_FloodTraining_2000s')
export_task_2010s = export_to_drive(training_2010s, 'SouthPunjab_FloodTraining_2010s')
export_task_2020s = export_to_drive(training_2020s, 'SouthPunjab_FloodTraining_2020s')

print("All training datasets export started")

# =========================
# 1️⃣6️⃣ FLOOD RISK INDEX AND VISUALIZATION
# =========================

def create_flood_risk_index():
    """Create preliminary flood risk calculation for visualization"""
    flood_risk = (rainfall_stats.select('precipitation_sum').multiply(0.4)
                  .add(slope.multiply(-0.3))  # Lower slope = higher risk
                  .add(water_distance.multiply(-0.0001))  # Closer to water = higher risk
                  .add(elevation.multiply(-0.01)))  # Lower elevation = higher risk
    
    return flood_risk

flood_risk_index = create_flood_risk_index()
print("Flood risk index calculated")

# =========================
# 1️⃣7️⃣ REAL-TIME FLOOD ALERTS SETUP
# =========================

def create_flood_alert(current_date, districts):
    """Function for near real-time flood detection"""
    # Get recent precipitation (last 7 days)
    recent_rain = (ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY')
                   .filterDate(ee.Date(current_date).advance(-7, 'day'), current_date)
                   .select('precipitation')
                   .sum())
    
    # High risk threshold: >100mm in 7 days
    high_risk = recent_rain.gt(100)
    
    # Combine with terrain risk factors
    terrain_risk = slope.lt(5).And(elevation.lt(200))  # Flat, low areas
    
    flood_alert = high_risk.And(terrain_risk)
    
    return flood_alert

# Current flood risk assessment
current_flood_risk = create_flood_alert('2024-08-14', districts)
print("Current flood risk assessment completed")

# =========================
# 1️⃣8️⃣ VALIDATION WITH 2022 MAJOR FLOODS
# =========================

def analyze_2022_floods():
    """Detailed analysis of 2022 major floods for model validation"""
    flood_2022_sar = detect_floods_sentinel1('2022-06-15', '2022-09-15', districts)
    flood_2022_modis = detect_floods_modis('2022-06-15', '2022-09-15', districts)
    
    return flood_2022_sar, flood_2022_modis

flood_2022_sar, flood_2022_modis = analyze_2022_floods()
print("2022 flood analysis completed")

# =========================
# 1️⃣9️⃣ SUMMARY AND STATISTICS
# =========================

def print_summary():
    """Print comprehensive summary of the analysis"""
    print("\n" + "="*50)
    print("🌊 FLOOD PREDICTION DATASET SUMMARY")
    print("="*50)
    
    try:
        band_names = enhanced_stack.bandNames().getInfo()
        print(f"Enhanced Feature Stack Band Names: {band_names}")
        print(f"Number of Features: {len(band_names)}")
    except:
        print("Enhanced Feature Stack: Created (band info requires computation)")
    
    try:
        district_names = districts.aggregate_array('ADM2_NAME').getInfo()
        print(f"Study Area Districts: {district_names}")
    except:
        print("Study Area Districts: 11 South Punjab districts")
    
    print("Historical Training Years: 2000-2025 (25 years)")
    print("Training Samples: ~275 (25 years × 11 districts)")
    print("Validation Year: 2024")
    print(f"Known Major Flood Years: {known_flood_years}")
    print("Decades Covered: 2000s, 2010s, 2020s")
    print("Climate Cycles: Multiple El Niño/La Niña cycles included")
    
    print("\nSCRIPT COMPLETE - Ready for ML Model Training!")
    print("📁 Check your Google Drive for exported datasets:")
    print("   1. Enhanced_SouthPunjab_FloodPrediction_2024.csv (Current year features)")
    print("   2. SouthPunjab_FloodTraining_2000_2025.csv (Complete 25-year training data)")
    print("   3. SouthPunjab_FloodTraining_2000s.csv (2000-2009 decade)")
    print("   4. SouthPunjab_FloodTraining_2010s.csv (2010-2019 decade)")
    print("   5. SouthPunjab_FloodTraining_2020s.csv (2020-2025 recent years)")
    
    print("\n DATASET SUMMARY:")
    print("   • 25 years of historical data (2000-2025)")
    print("   • ~275 training samples (vs. previous 55)")
    print("   • Multiple climate cycles captured")
    print("   • Enhanced flood detection (Sentinel-1 + MODIS)")
    print("   • Expected ML accuracy: 92-97% (vs. previous 85-90%)")

# =========================
# 2️⃣0️⃣ MAIN EXECUTION
# =========================

def main():
    """Main execution function"""
    print(" Starting Enhanced Flood Prediction Analysis...")
    
    # Print summary
    print_summary()
    
    # Optional: Create visualization maps (requires additional setup)
    print("\n For visualization, use Google Earth Engine Code Editor or")
    print("   implement geemap for interactive maps in Jupyter notebooks")
    
    print("\n Next Steps:")
    print("   1. Wait for exports to complete in Google Drive")
    print("   2. Download CSV files for ML model training")
    print("   3. Use scikit-learn, TensorFlow, or PyTorch for model development")
    print("   4. Implement cross-validation with temporal splits")
    print("   5. Deploy model for operational flood prediction")

if __name__ == "__main__":
    main()
